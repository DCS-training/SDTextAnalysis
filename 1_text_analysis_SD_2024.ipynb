{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis\n",
    "\n",
    "## Python Variables and Text Strings\n",
    "\n",
    "**Text String:** A text string is a set of words or characters contained in double or single quotes.\n",
    "\n",
    "**Variable:** A variable is something that holds a value that may change. In simplest terms, a variable is just a box that you can put stuff in. W are going to look at storing text strings in variables.\n",
    "\n",
    "Here is an example a text string:\n",
    "\n",
    "\"All of our technology is completely unnecessary to a happy life.\"\n",
    "\n",
    "We can assign this text string to a variable - in this case a variable called called 'quote':\n",
    "\n",
    "The benifit of storing the text string in a variable means that when we write code to manipulate the text in some way we only have to refer to the variable rather than typing the complete text.\n",
    "\n",
    "\n",
    "### First install necessary additional Python packages\n",
    "Run the following piece of code by selecting its cell and clicking the 'Run' button on the toolbar. This will only needs to be done the first time you use this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk #Install NLTK Library \n",
    "!pip install matplotlib #Install the Matplotlib Library \n",
    "!pip install urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the needed libraries\n",
    "\n",
    "The following code will import the libraries we need. \n",
    "N.B. Urllib, Urllib3, and Re should be already installed but if you are running this notebook on your own machine and you get an error you may need to install those libraries using !pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #this is the main Text Analysis Library\n",
    "nltk.download('stopwords') #From the main library we download the stopwords\n",
    "nltk.download('punkt')#From the main library we download the tokeniser\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize # Import the tokeniser by words\n",
    "from nltk.tokenize import sent_tokenize # Import the tokeniser by sentences\n",
    "from nltk.corpus import stopwords # Import the stopwords function\n",
    "from nltk.probability import FreqDist # class for making frequency distributions\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "# import urllib # We need this library to read Urls\n",
    "import urllib3 # We need this library to read Urlsimport csv # Import csv Library\n",
    "import csv # Import csv Library\n",
    "import re # We need this Library to use Regex\n",
    "import matplotlib.pyplot as plt\n",
    "import string, collections # utility libraries for strings and collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following piece of code by selecting its cell and clicking the 'Run' button on the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"So, Faustus, what wouldst thou have me do?\"\n",
    "print(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Let's now change the case of the string.\n",
    "The following code takes the variable 'quote' and changes the text to uppercase using the upper() method. It then assigns the results to a new variable called 'big_quote'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_quote = quote.upper()\n",
    "print(big_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is desireable to get rid of all capitalization completely so that Python does not distinguish between, for examle, 'Office' and 'office'.\n",
    "\n",
    "### Change all words in the text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_quote = quote.lower()\n",
    "print(lower_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining strings (concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_name = \"ada\"\n",
    "last_name = \"lovelace\"\n",
    "\n",
    "full_name = first_name + ' ' + last_name # combine strings with a space inbetween\n",
    "\n",
    "print(full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence case\n",
    "However, we would want to capitalise first names and last names when we output them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_name = full_name.title() # convert name to sentence case\n",
    "\n",
    "print(proper_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Handling \n",
    "\n",
    "### Working with a text file\n",
    "\n",
    "### Option A Import from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = \"https://raw.githubusercontent.com/DCS-training/IntroToTextAnalysis/main/darwin-origin.txt\" #read the URL\n",
    "http     = urllib3.PoolManager() #Create and object\n",
    "response = http.request('GET', file_url) #Attempt to get the content\n",
    "data     = response.data.decode('utf-8') #encode the results\n",
    "print(data[:1076]) #Print sample of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B Import an uploaded file \n",
    "\n",
    "Download the following file to your computer (The Introduction to Origin of Species by Charles Darwin)\n",
    "\n",
    "<a href=\"https://raw.githubusercontent.com/DCS-training/IntroToTextAnalysis/main/darwin-origin.txt\" download>https://raw.githubusercontent.com/DCS-training/IntroToTextAnalysis/main/darwin-origin.txt</a>\n",
    "\n",
    "Right-click on the link and select 'Save Link As..'. Save the file to your Downloads directory or to another appropriate location on your computer.\n",
    "\n",
    "Once the file has been saved, go back to the Noteable home tab in the browser. \n",
    "* Select 'Upload' from the top right of the page. \n",
    "* Browse to the file.\n",
    "* Click 'Select'\n",
    "* Click on the blue 'Upload' button\n",
    "\n",
    "The file is now available to be used in Noteable (you should be able to see it in the file list under the home tab)\n",
    "\n",
    "We can open the file for reading with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"darwin-origin.txt\") as file:\n",
    "    txt=file.read()\n",
    "print(txt[:1076]) #Print sample of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the file contents to a variable\n",
    "\n",
    "Now we have opened the file we can assign the contents of the file to a variable ('txt' in this case).\n",
    "\n",
    "The following code block:\n",
    "* Assigns the text to a variable\n",
    "* Transforms all the text to lower case\n",
    "* Prints the contens of the variable to the screen\n",
    "\n",
    "Experiment with printing out differing numbers of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = data.lower()\n",
    "print(txt[:500]) # Restrict to first 500 characters\n",
    "#print (txt[-500:]) # This would display the last 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clear this text from the console. \n",
    "\n",
    "From the top menu, select Cell > Current Outputs > Clear\n",
    "\n",
    "### Removing punctuation\n",
    "\n",
    "It will also be necessary in many cases to remove punctuation.  If you get an error re run the second cell \"Import needed Libraries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation. It is using Regex\n",
    "print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition.\n",
    "\n",
    "### Tokenization\n",
    "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. A Token is a single entity that is building blocks for sentence or paragraph.\n",
    "\n",
    "### Sentence Tokenization\n",
    "\n",
    "Sentence tokenizen breaks text paragraph into sentences.\n",
    "\n",
    "Run the following to split the file origin-intro.txt into sentences we are going to use the sent_tokenize we imported in the second cell. If you get an error re run the second cell \"Import needed Libraries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_text=sent_tokenize(txt)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "\n",
    "Word tokenizer breaks text paragraph into words.\n",
    "\n",
    "Run the following to split the file origin-intro.txt into words.  If you get an error re run the second cell \"Import needed Libraries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_text=word_tokenize(txt)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Often when analyzing text we want to remove common words that occur multiple times but for our purposes just constitute 'noise. You can see this in the output of the code block above. NLTK can help with this by providing a ready-made list of these words (called 'stopwords') that can then be filtered out.\n",
    "\n",
    "Run the following block of code to see this list.  If you get an error re run the second cell \"Import needed Libraries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text\n",
    "\n",
    "Putting these steps together we can\n",
    "* Convert to lower case\n",
    "* Split into tokens\n",
    "* remove stopwords\n",
    "\n",
    "Run the following to see the results of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = data.lower()  # lower case text\n",
    "\n",
    "contents = re.sub(r'[^\\w\\s]','',contents)  #remove punctuation\n",
    "\n",
    "tokenized_word=word_tokenize(txt)\n",
    "\n",
    "filtered_word=[]\n",
    "\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_word.append(w)\n",
    "print(\"Filterd Words:\",filtered_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count\n",
    "\n",
    "The following command will return a count of the tokens returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n",
    "\n",
    "The following code will generate a frequency distribution which basically counts all the unique words in your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(filtered_word)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words\n",
    "\n",
    "You can also generate a list of the most common words. The following code is set to output the top 10 - experiment by changing this to a different number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution Plot\n",
    "\n",
    "You can also generate a graph to provide a visual representation of frequency - again experiment by changing the number (25 in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fdist.plot(25,cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging\n",
    "\n",
    "The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "\n",
    "As an example we'll identify the parts of speech in the quote we used earlier (feel free to change this to an example of your own)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"My name is Ozymandias, king of kings. Look upon my works, ye mighty, and despair\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we  split the text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=nltk.word_tokenize(quote)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we run the following code identify parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common POS Tags\n",
    "\n",
    "\n",
    "| Tag      | Part of Speech |\n",
    "| ------------- | -----|\n",
    "| DT | determiner |\n",
    "| JJ | adjective |\n",
    "| IN | preposition |\n",
    "| NN | noun singular |\n",
    "| NNS | noun plural |\n",
    "| PRP$ | possessive pronoun |\n",
    "| RB | adverb |\n",
    "| VB | verb |\n",
    "| VBZ | verb, 3rd person sing |\n",
    "\n",
    "#### For reference, a complete list of tags can be obtained by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams and common word pairs\n",
    "\n",
    "N-grams are simply all combinations of adjacent words length N that you can find in your source text. For instance:\n",
    "\n",
    "• San Francisco (is a 2-gram)\n",
    "\n",
    "• The Three Musketeers (is a 3-gram)\n",
    "\n",
    "• She stood up slowly (is a 4-gram)\n",
    "\n",
    "We can use the **ngram** function in **nltk** to identify the most frequent n-grams in the text file we just created\n",
    "\n",
    "This code will obtain the most common 2 word sequences.\n",
    "\n",
    "The number of words (N) is set to 2 with the following line in the code:\n",
    "\n",
    "`N = 2`\n",
    "\n",
    "Change this to another number to experiment with different N-grams\n",
    "\n",
    "In the following example we will use the NLTK ngram function to find the most common word pairs (bi-grams).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"darwin-origin.txt\") as file: # open file\n",
    "    txt = file.read()          # add file contents to variable\n",
    "txt = txt.lower()  # lower case text\n",
    "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Apply the stopwords to the text\n",
    "txt = [word for word in txt.split() if word.lower() not in stop_words]\n",
    "txt = ' '.join(txt).lower()\n",
    "\n",
    "# get rid of punctuation\n",
    "remove = string.punctuation\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "text = (re.sub(pattern, \"\", txt))\n",
    "\n",
    "# first get individual words\n",
    "tokenized = text.split()\n",
    "\n",
    "# and get a list of all the bi-grams\n",
    "words = ngrams(tokenized, 2)\n",
    "\n",
    "# get the frequency of each bigram in the text\n",
    "wordsFreq = collections.Counter(words)\n",
    "\n",
    "for k,v in wordsFreq.most_common(30):\n",
    "    k = ' '.join(k)\n",
    "    print(k,v) \n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-grams\n",
    "\n",
    "Now let's look for the most common 3-word phrases:\n",
    "\n",
    "In the code above find the line:\n",
    "\n",
    "`words = ngrams(tokenized, 2)`\n",
    "\n",
    "And change it to:\n",
    "\n",
    "`words = ngrams(tokenized, 3)`\n",
    "\n",
    "Run the code again to see the results\n",
    "\n",
    "You can also change the number of ngrams returned. The line:\n",
    "\n",
    "`for k,v in wordsFreq.most_common(20):`\n",
    "\n",
    "Sets the result set to `20`. Try experimenting by changing this number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and writing CSV files \n",
    "\n",
    "We will use the following code to download a CSV and write to local CSV file.\n",
    "\n",
    "In this case the file is a CSV file containing all of Donald Trump's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/DCS-training/IntroToTextAnalysis/main/trump-tweet-archive.csv' # download the file\n",
    "\n",
    "response = urllib3.request(\"GET\", url) # assign the contents of the file to a variable (csv)\n",
    "\n",
    "with open('tweets.csv', 'wb') as file: # create a new file and save the contents of 'csv' to this file\n",
    "    file.write(response.data)\n",
    "    \n",
    "    print('CSV file created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect our new file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import csv\n",
    "\n",
    "with open('tweets.csv', 'r', encoding=\"utf8\") as csv_file: # open the csv data file\n",
    "    reader = csv.reader(csv_file)\n",
    "\n",
    "# import the last 5 tweets\n",
    "    cnt = 0\n",
    "    for row in reader:\n",
    "        if cnt < 5:\n",
    "            print(row[-5:])\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write Tweets to text file\n",
    "For the next step we need to export the tweet text and write it to a text file\n",
    "\n",
    "• We will ignore the tweet id and the date\n",
    "\n",
    "• We will exclude any retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweets.csv', 'r', encoding=\"utf8\") as csv_file, open('tweets.txt','w', encoding=\"utf8\") as text_file: # open the csv data file\n",
    "    next(csv_file, None)  # skip the header row\n",
    "    reader = csv.reader(csv_file)\n",
    "\n",
    "\n",
    "    for row in reader:\n",
    "\n",
    "        tweet = row[2]\n",
    "        if ('RT @' not in tweet): # Exclude retweets\n",
    "            text_file.write(tweet + '\\n')\n",
    "\n",
    "\n",
    "print(\"Tweets written to 'tweets.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect our new text file\n",
    "\n",
    "### Print the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tweets.txt\", encoding=\"utf8\") as txt: # open file\n",
    "    lines = txt.readlines()\n",
    "\n",
    "first_lines = lines[:10]\n",
    "\n",
    "print(first_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now print the last 10 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_lines = lines[-10:]\n",
    "\n",
    "print(first_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "We can use the ngram function in nltk to identify the most frequent n-grams in the text file we just created.\n",
    "\n",
    "You will see on line 4 in the code we import 'stopwords'. This is a list of the most common words that we wish to exclude with any analysis.\n",
    "\n",
    "As before the number of n-grams and the value of <i>n</i> can be changed - just change the numbers in the code below where indicated in the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tweets.txt\", encoding=\"utf8\") as source_file: # open file\n",
    "    txt = source_file.read()  # add file contents to variable\n",
    "\n",
    "txt = txt.lower()  # lower case text\n",
    "\n",
    "stop_words = stopwords.words('english') \n",
    "\n",
    "# Apply the stopwords to the text\n",
    "txt = [word for word in txt.split() if word not in stop_words]\n",
    "\n",
    "# and get a list of all the bi-grams\n",
    "pairs = ngrams(txt, 2) # Change this number to see different n-grams\n",
    "\n",
    "# get the frequency of each bigram in the text\n",
    "pairsFreq = collections.Counter(pairs)\n",
    "\n",
    "for k, v in pairsFreq.most_common(20): # Change this number to change the number of n-grams\n",
    "    k = ' '.join(k)\n",
    "    print(k, v)\n",
    "\n",
    "source_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exercises\n",
    "\n",
    "Experiment by analysing different text files. A selection can be found on the workshop webpage (or use a file of you own choosing):\n",
    "\n",
    "[https://github.com/DCS-training/IntroToTextAnalysis/](https://github.com/DCS-training/IntroToTextAnalysis/)\n",
    "\n",
    "Once the file has been saved to your computer, go back to the Noteable home tab in the browser.\n",
    "\n",
    "* Select 'Upload' from the top right of the page. \n",
    "* Browse to the file.\n",
    "* Click 'Select'\n",
    "* Click on the blue 'Upload' button\n",
    "\n",
    "The file is now available to be used in Noteable.\n",
    "\n",
    "In the code blocks replace `origin-intro.txt` with the name of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fda35ce31702043eb00f71f47e90c41af5511afe6fb73e723507c852808de0cc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
