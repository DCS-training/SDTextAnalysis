{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3HnFWgxO6xz"
      },
      "source": [
        "# Introduction to Text Analysis\n",
        "\n",
        "## Python Variables and Text Strings\n",
        "\n",
        "**Text String:** A text string is a set of words or characters contained in double or single quotes.\n",
        "\n",
        "**Variable:** A variable is something that holds a value that may change. In simplest terms, a variable is just a box that you can put stuff in. W are going to look at storing text strings in variables.\n",
        "\n",
        "Here is an example a text string:\n",
        "\n",
        "\"All of our technology is completely unnecessary to a happy life.\"\n",
        "\n",
        "We can assign this text string to a variable - in this case a variable called called 'quote':\n",
        "\n",
        "The benifit of storing the text string in a variable means that when we write code to manipulate the text in some way we only have to refer to the variable rather than typing the complete text.\n",
        "\n",
        "\n",
        "### First install necessary additional Python packages\n",
        "Run the following piece of code by selecting its cell and clicking the 'Run' button on the toolbar. This will only needs to be done the first time you use this Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfAP6onzO6x7",
        "outputId": "6931bd8d-143f-4e5c-f2e6-6cad47506bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk #Install NLTK Library\n",
        "!pip install matplotlib #Install the Matplotlib Library\n",
        "!pip install urllib3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjR1T9L5O6x_"
      },
      "source": [
        "## Import the needed libraries\n",
        "\n",
        "The following code will import the libraries we need.\n",
        "N.B. Urllib, Urllib3, and Re should be already installed but if you are running this notebook on your own machine and you get an error you may need to install those libraries using !pip install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSZm_q4_O6yA"
      },
      "outputs": [],
      "source": [
        "import nltk #this is the main Text Analysis Library\n",
        "nltk.download('stopwords') #From the main library we download the stopwords\n",
        "nltk.download('punkt')#From the main library we download the tokeniser\n",
        "from nltk.tokenize import word_tokenize # Import the tokeniser by words\n",
        "from nltk.tokenize import sent_tokenize # Import the tokeniser by sentences\n",
        "from nltk.corpus import stopwords # Import the stopwords function\n",
        "import urllib # We need this library to read Urls\n",
        "import urllib3 # We need this library to read Urls\n",
        "import re # We need this Library to use Regex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjzEFmeuO6yB"
      },
      "source": [
        "### Run the following piece of code by selecting its cell and clicking the 'Run' button on the toolbar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72a9GIGxO6yB"
      },
      "outputs": [],
      "source": [
        "quote = \"All of our technology is completely unnecessary to a happy life.\"\n",
        "print(quote)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNpD9E1mO6yC"
      },
      "source": [
        "\n",
        "### Let's now change the case of the string.\n",
        "The following code takes the variable 'quote' and changes the text to uppercase using the upper() method. It then assigns the results to a new variable called 'big_quote'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZW5GvyvO6yE"
      },
      "outputs": [],
      "source": [
        "big_quote = quote.upper()\n",
        "print(big_quote)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HzST7C2O6yE"
      },
      "source": [
        "Often it is desireable to get rid of all capitalization completely so that Python does not distinguish between, for examle, 'Office' and 'office'.\n",
        "\n",
        "### Change all words in the text to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrcYdPRoO6yF"
      },
      "outputs": [],
      "source": [
        "lower_quote = quote.lower()\n",
        "print(lower_quote)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2gH72_xO6yG"
      },
      "source": [
        "### Combining strings (concatenation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MEl-1ogO6yH"
      },
      "outputs": [],
      "source": [
        "first_name = \"ada\"\n",
        "last_name = \"lovelace\"\n",
        "\n",
        "full_name = first_name + ' ' + last_name # combine strings with a space inbetween\n",
        "\n",
        "print(full_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY55hRwIO6yH"
      },
      "source": [
        "### Sentence case\n",
        "However, we would want to capitalise first names and last names when we output them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpaXYOL3O6yH"
      },
      "outputs": [],
      "source": [
        "proper_name = full_name.title() # convert name to sentence case\n",
        "\n",
        "print(proper_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WRt19pEO6yH"
      },
      "source": [
        "# File Handling\n",
        "\n",
        "### Working with a text file\n",
        "\n",
        "### Option A Import from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnbKF_ELO6yI"
      },
      "outputs": [],
      "source": [
        "file_url = \"https://raw.githubusercontent.com/DCS-training/IntroToTextAnalysis/main/darwin-origin.txt\" #read the URL\n",
        "http     = urllib3.PoolManager() #Create and object\n",
        "response = http.request('GET', file_url) #Attempt to get the content\n",
        "data     = response.data.decode('utf-8') #encode the results\n",
        "print(data[:1076]) #Print sample of results\n",
        "with open('darwin-origin.txt', 'w') as f: f.write('darwin-origin') # Save the file in the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en-CIGHhO6yI"
      },
      "source": [
        "### Option B Import an uploaded file\n",
        "\n",
        "Download the following file to your computer (The Introduction to Origin of Species by Charles Darwin)\n",
        "\n",
        "<a href=\"https://raw.githubusercontent.com/DCS-training/IntroToTextAnalysis/main/darwin-origin.txt\" download>https://raw.githubusercontent.com/DCS-training/IntroToTextAnalysis/main/darwin-origin.txt</a>\n",
        "\n",
        "Right-click on the link and select 'Save Link As..'. Save the file to your Downloads directory or to another appropriate location on your computer.\n",
        "\n",
        "Once the file has been saved, use the following cell to import it to Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQEqoIxWQDHg"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0-mHkjkO6yK"
      },
      "source": [
        "### Assign the file contents to a variable\n",
        "\n",
        "Now we have opened the file we can assign the contents of the file to a variable ('txt' in this case).\n",
        "\n",
        "The following code block:\n",
        "* Assigns the text to a variable\n",
        "* Transforms all the text to lower case\n",
        "* Prints the contens of the variable to the screen\n",
        "\n",
        "Experiment with printing out differing numbers of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dti4mZIO6yK"
      },
      "outputs": [],
      "source": [
        "txt = data.lower()\n",
        "print(txt[:500]) # Restrict to first 500 characters\n",
        "#print (txt[-500:]) # This would display the last 500 characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpAYlqhOO6yL"
      },
      "source": [
        "You can clear this text from the console.\n",
        "\n",
        "From the top menu, select Cell > Current Outputs > Clear\n",
        "\n",
        "### Removing punctuation\n",
        "\n",
        "It will also be necessary in many cases to remove punctuation.  If you get an error re run the second cell \"Import needed Libraries\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq3aMCwqO6yL",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation. It is using Regex\n",
        "print (txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SEwRlp0O6yM"
      },
      "source": [
        "## NLTK\n",
        "\n",
        "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition.\n",
        "\n",
        "### Tokenization\n",
        "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. A Token is a single entity that is building blocks for sentence or paragraph.\n",
        "\n",
        "### Sentence Tokenization\n",
        "\n",
        "Sentence tokenizen breaks text paragraph into sentences.\n",
        "\n",
        "Run the following to split the file origin-intro.txt into sentences we are going to use the sent_tokenize we imported in the second cell. If you get an error re run the second cell \"Import needed Libraries\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXEuV21vO6yN",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "tokenized_text=sent_tokenize(txt)\n",
        "\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_-sG3wQO6yN"
      },
      "source": [
        "### Word Tokenization\n",
        "\n",
        "Word tokenizer breaks text paragraph into words.\n",
        "\n",
        "Run the following to split the file origin-intro.txt into words.  If you get an error re run the second cell \"Import needed Libraries\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3CEWq8-O6yO",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenized_text=word_tokenize(txt)\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVjXP3zvO6yO"
      },
      "source": [
        "## Stopwords\n",
        "\n",
        "Often when analyzing text we want to remove common words that occur multiple times but for our purposes just constitute 'noise. You can see this in the output of the code block above. NLTK can help with this by providing a ready-made list of these words (called 'stopwords') that can then be filtered out.\n",
        "\n",
        "Run the following block of code to see this list.  If you get an error re run the second cell \"Import needed Libraries\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPICVZXTO6yP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "stop_words=set(stopwords.words(\"english\"))\n",
        "\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r3pWY5LO6yP"
      },
      "source": [
        "## Cleaning the text\n",
        "\n",
        "Putting these steps together we can\n",
        "* Convert to lower case\n",
        "* Split into tokens\n",
        "* remove stopwords\n",
        "\n",
        "Run the following to see the results of this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFzp3HOgO6yQ",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "contents = data.lower()  # lower case text\n",
        "\n",
        "contents = re.sub(r'[^\\w\\s]','',contents)  #remove punctuation\n",
        "\n",
        "tokenized_word=word_tokenize(txt)\n",
        "\n",
        "filtered_word=[]\n",
        "\n",
        "for w in tokenized_word:\n",
        "    if w not in stop_words:\n",
        "        filtered_word.append(w)\n",
        "print(\"Filterd Words:\",filtered_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6no5SP2O6yQ"
      },
      "source": [
        "### Word Count\n",
        "\n",
        "The following command will return a count of the tokens returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewYVaiS-O6yQ"
      },
      "outputs": [],
      "source": [
        "print(len(filtered_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7KJIRwDO6yR"
      },
      "source": [
        "### Frequency Distribution\n",
        "\n",
        "The following code will generate a frequenc distribution which basically counts all the unique words in your text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN1mN97GO6yR"
      },
      "outputs": [],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "fdist = FreqDist(filtered_word)\n",
        "print(fdist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7oZhwmfO6yS"
      },
      "source": [
        "### Most common words\n",
        "\n",
        "You can also generate a list of the most common words. The following code is set to output the top 10 - experiment by changing this to a different number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdjdtiQsO6yS"
      },
      "outputs": [],
      "source": [
        "fdist.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd0ccNKYO6yT"
      },
      "source": [
        "### Frequency Distribution Plot\n",
        "\n",
        "You can also generate a graph to provide a visual representation of frequency - again experiment by changing the number (25 in this example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4n5u5UVO6yT"
      },
      "outputs": [],
      "source": [
        "# Frequency Distribution Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "fdist.plot(25,cumulative=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ImO-FepO6yU"
      },
      "source": [
        "## Parts of Speech Tagging\n",
        "\n",
        "The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
        "\n",
        "As an example we'll identify the parts of speech in the quote we used earlier (feel free to change this to an example of your own)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCC0WsdHO6yf"
      },
      "outputs": [],
      "source": [
        "quote = \"All of our technology is completely unnecessary to a happy life\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceoPxXsQO6yf"
      },
      "source": [
        "#### First we  split the text into tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOsXxzYNO6yg"
      },
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "tokens=nltk.word_tokenize(quote)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRacIjKWO6yh"
      },
      "source": [
        "#### Then we run the following code identify parts of speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUZNEonTO6yi"
      },
      "outputs": [],
      "source": [
        "nltk.pos_tag(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJ4Rxs-O6yi"
      },
      "source": [
        "#### Common POS Tags\n",
        "\n",
        "\n",
        "| Tag      | Part of Speech |\n",
        "| ------------- | -----|\n",
        "| DT | determiner |\n",
        "| JJ | adjective |\n",
        "| IN | preposition |\n",
        "| NN | noun singular |\n",
        "| NNS | noun plural |\n",
        "| PRP$ | possessive pronoun |\n",
        "| RB | adverb |\n",
        "| VB | verb |\n",
        "| VBZ | verb, 3rd person sing |\n",
        "\n",
        "#### For reference, a complete list of tags can be obtained by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahEUL-8mO6yj"
      },
      "outputs": [],
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGMK7S1UO6yj"
      },
      "source": [
        "### N-Grams and common word pairs\n",
        "\n",
        "N-grams are simply all combinations of adjacent words length N that you can find in your source text.\n",
        "\n",
        "In the following example we will use the NLTK ngram function to find the most common word pairs (bi-grams).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCayU-2iO6yk"
      },
      "outputs": [],
      "source": [
        "import nltk, re, string, collections # Making sure we have all the Libraries we need\n",
        "from nltk.util import ngrams # function for making ngrams\n",
        "\n",
        "file = open(\"darwin-origin.txt\") # open file\n",
        "txt = file.read()          # add file contents to variable\n",
        "txt = txt.lower()  # lower case text\n",
        "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=set(stopwords.words(\"english\"))\n",
        "\n",
        "# Apply the stopwords to the text\n",
        "txt = [word for word in txt.split() if word.lower() not in stopwords]\n",
        "txt = ' '.join(txt).lower()\n",
        "\n",
        "# get rid of punctuation\n",
        "remove = string.punctuation\n",
        "pattern = r\"[{}]\".format(remove) # create the pattern\n",
        "text = (re.sub(pattern, \"\", txt))\n",
        "\n",
        "# first get individual words\n",
        "tokenized = text.split()\n",
        "\n",
        "# and get a list of all the bi-grams\n",
        "words = ngrams(tokenized, 2)\n",
        "\n",
        "# get the frequency of each bigram in the text\n",
        "wordsFreq = collections.Counter(words)\n",
        "\n",
        "for k,v in wordsFreq.most_common(30):\n",
        "    k = ' '.join(k)\n",
        "    print(k,v)\n",
        "\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xafSHG1O6yl"
      },
      "source": [
        "### Tri-grams\n",
        "\n",
        "Now let's look for the most common 3-word phrases:\n",
        "\n",
        "In the code above find the line:\n",
        "\n",
        "`words = ngrams(tokenized, 2)`\n",
        "\n",
        "And change it to:\n",
        "\n",
        "`words = ngrams(tokenized, 3)`\n",
        "\n",
        "Run the code again to see the results\n",
        "\n",
        "You can also change the number of ngrams returned. The line:\n",
        "\n",
        "`for k,v in wordsFreq.most_common(20):`\n",
        "\n",
        "Sets the result set to `20`. Try experimenting by changing this number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyJiuogPO6yl"
      },
      "source": [
        "### Reading and writing CSV files\n",
        "\n",
        "We will use the following code to download a CSV and write to local CSV file.\n",
        "\n",
        "In this case the file is a CSV file containing all of Donald Trump's tweets.\n",
        "\n",
        "Download this file 'https://raw.githubusercontent.com/DCS-training/IntroToTextAnalysis/main/trump-tweet-archive.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsxsWVpTSMuK"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJTC0VySO6yn"
      },
      "source": [
        "## Inspect our new file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6cIkQLrO6yn",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "import csv # Import csv Library\n",
        "#import csv\n",
        "\n",
        "csv_file = open('trump-tweet-archive.csv', 'r', encoding=\"utf8\")  # open the csv data file\n",
        "reader = csv.reader(csv_file)\n",
        "\n",
        "# import the last 5 tweets\n",
        "cnt = 0\n",
        "for row in reader:\n",
        "    if cnt < 5:\n",
        "        print(row[-5:])\n",
        "        cnt += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exA5L-mBO6yo",
        "tags": []
      },
      "source": [
        "## Write Tweets to text file\n",
        "For the next step we need to export the tweet text and write it to a text file\n",
        "\n",
        "• We will ignore the tweet id and the date\n",
        "\n",
        "• We will exclude any retweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "283PivSTO6yq"
      },
      "outputs": [],
      "source": [
        "csv_file = open('trump-tweet-archive.csv', 'r', encoding=\"utf8\")  # open the csv data file\n",
        "next(csv_file, None)  # skip the header row\n",
        "reader = csv.reader(csv_file)\n",
        "\n",
        "# create/open output file\n",
        "text_file = open('tweets.txt','w', encoding=\"utf8\")\n",
        "\n",
        "\n",
        "for row in reader:\n",
        "\n",
        "    tweet = row[2]\n",
        "    if ('RT @' not in tweet): # Exclude retweets\n",
        "       text_file.write(tweet + '\\n')\n",
        "\n",
        "csv_file.close()\n",
        "text_file.close()\n",
        "\n",
        "print(\"Tweets written to 'tweets.txt'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDN90OLHO6yq"
      },
      "source": [
        "# Inspect our new text file\n",
        "\n",
        "### Print the first 10 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tKqnnLnO6yr"
      },
      "outputs": [],
      "source": [
        "txt = open(\"tweets.txt\", encoding=\"utf8\")  # open file\n",
        "\n",
        "lines = txt.readlines()\n",
        "first_lines = lines[:10]\n",
        "\n",
        "print(first_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROicDMBmO6ys"
      },
      "source": [
        "### Now print the last 10 lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUi3Pv2wO6yt"
      },
      "outputs": [],
      "source": [
        "txt = open(\"tweets.txt\", encoding=\"utf8\")  # open file\n",
        "\n",
        "lines = txt.readlines()\n",
        "first_lines = lines[-10:]\n",
        "\n",
        "print(first_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzmOlxdKO6yu"
      },
      "source": [
        "## N-grams\n",
        "\n",
        "We can use the ngram function in nltk to identify the most frequent n-grams in the text file we just created.\n",
        "\n",
        "As before the number of n-grams can be changed - jus change the number in the code below where indicated in the comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg9t8I4dO6yu"
      },
      "outputs": [],
      "source": [
        "import nltk, re, string, collections # Make sure that we have all the packages we need\n",
        "from nltk.util import ngrams  # function for making ngrams\n",
        "from nltk.corpus import stopwords # We already imported before but just to be sure\n",
        "\n",
        "source_file = open(\"tweets.txt\", encoding=\"utf8\")  # open file\n",
        "txt = source_file.read()  # add file contents to variable\n",
        "txt = txt.lower()  # lower case text\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Apply the stopwords to the text\n",
        "txt = [word for word in txt.split() if word.lower() not in stop_words]\n",
        "#txt = ' '.join(txt).lower()\n",
        "\n",
        "# and get a list of all the bi-grams\n",
        "pairs = ngrams(txt, 2) # Change this number to see different n-grams\n",
        "\n",
        "# get the frequency of each bigram in the text\n",
        "pairsFreq = collections.Counter(pairs)\n",
        "\n",
        "for k, v in pairsFreq.most_common(20): # Change this number to change the number of n-grams\n",
        "    k = ' '.join(k)\n",
        "    print(k, v)\n",
        "\n",
        "source_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdggoWAIO6yv"
      },
      "source": [
        "## Further Exercises\n",
        "\n",
        "Experiment by analysing different text files. A selection can be found on the workshop webpage (or use a file of you own choosing):\n",
        "\n",
        "[https://github.com/DCS-training/IntroToTextAnalysis/](https://github.com/DCS-training/IntroToTextAnalysis/)\n",
        "\n",
        "Once the file has been saved to your computer, go back to the Noteable home tab in the browser.\n",
        "\n",
        "* Select 'Upload' from the top right of the page.\n",
        "* Browse to the file.\n",
        "* Click 'Select'\n",
        "* Click on the blue 'Upload' button\n",
        "\n",
        "The file is now available to be used in Noteable.\n",
        "\n",
        "In the code blocks replace `origin-intro.txt` with the name of your file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sZcVzk1O6yv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "fda35ce31702043eb00f71f47e90c41af5511afe6fb73e723507c852808de0cc"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
